{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv')",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "75922e57d599accef8c0f8afabe40177a9b456aabcaf41c0a9a3b9df59c293a9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('SENTENCE', [Token(name='WORD', text='Linux', span=(0, 5)), Token(name='NUMBER', text='5', span=(6, 7)), Token(name='DOT', text='.', span=(7, 8)), Token(name='NUMBER', text='9', span=(8, 9)), Token(name='WORD', text='miał', span=(10, 14)), Token(name='WORD', text='być', span=(15, 18)), Token(name='WORD', text='mniejszą', span=(19, 27)), Token(name='WORD', text='aktualizacją', span=(28, 40)), Token(name='WORD', text='niż', span=(41, 44)), Token(name='WORD', text='ostatecznie', span=(45, 56)), Token(name='WORD', text='jest', span=(57, 61))])\n('SENTENCE', [Token(name='WORD', text='Na', span=(0, 2)), Token(name='WORD', text='większą', span=(3, 10)), Token(name='WORD', text='liczbę', span=(11, 17)), Token(name='WORD', text='nowinek', span=(18, 25)), Token(name='WORD', text='nikt', span=(26, 30)), Token(name='WORD', text='chyba', span=(31, 36)), Token(name='WORD', text='jednak', span=(37, 43)), Token(name='WORD', text='nie', span=(44, 47)), Token(name='WORD', text='będzie', span=(48, 54)), Token(name='WORD', text='narzekać', span=(55, 63))])\n('SENTENCE', [Token(name='WORD', text='Szczególnie', span=(0, 11)), Token(name='WORD', text='że', span=(13, 15)), Token(name='WORD', text='są', span=(16, 18)), Token(name='WORD', text='to', span=(19, 21)), Token(name='WORD', text='nowinki', span=(22, 29)), Token(name='WORD', text='nie', span=(30, 33)), Token(name='WORD', text='bez', span=(34, 37)), Token(name='WORD', text='znaczenia', span=(38, 47)), Token(name='WORD', text='takie', span=(49, 54)), Token(name='WORD', text='jak', span=(55, 58)), Token(name='LPAREN', text='(', span=(59, 60)), Token(name='WORD', text='dodana', span=(60, 66)), Token(name='WORD', text='po', span=(67, 69)), Token(name='WORD', text='ponad', span=(70, 75)), Token(name='NUMBER', text='5', span=(76, 77)), Token(name='WORD', text='latach', span=(78, 84)), Token(name='WORD', text='prób', span=(85, 89)), Token(name='RPAREN', text=')', span=(89, 90)), Token(name='WORD', text='obsługa', span=(91, 98)), Token(name='WORD', text='FSGSBASE', span=(99, 107)), Token(name='WORD', text='oznaczająca', span=(109, 120)), Token(name='WORD', text='spory', span=(121, 126)), Token(name='WORD', text='wzrost', span=(127, 133)), Token(name='WORD', text='wydajności', span=(134, 144)), Token(name='WORD', text='zarówno', span=(147, 154)), Token(name='WORD', text='w', span=(155, 156)), Token(name='WORD', text='przypadku', span=(157, 166)), Token(name='WORD', text='platform', span=(167, 175)), Token(name='WORD', text='opartych', span=(176, 184)), Token(name='WORD', text='na', span=(185, 187)), Token(name='WORD', text='procesorach', span=(188, 199)), Token(name='WORD', text='Intela', span=(200, 206)), Token(name='WORD', text='jak', span=(208, 211)), Token(name='WORD', text='i', span=(212, 213)), Token(name='WORD', text='tych', span=(214, 218)), Token(name='WORD', text='których', span=(220, 227)), Token(name='WORD', text='sercami', span=(228, 235)), Token(name='WORD', text='są', span=(236, 238)), Token(name='WORD', text='układy', span=(239, 245)), Token(name='WORD', text='AMD', span=(246, 249))])\n('SENTENCE', [Token(name='WORD', text='Najbardziej', span=(0, 11)), Token(name='WORD', text='odczujesz', span=(12, 21)), Token(name='WORD', text='to', span=(22, 24)), Token(name='WORD', text='w', span=(25, 26)), Token(name='WORD', text='sytuacjach', span=(27, 37)), Token(name='WORD', text='dużego', span=(38, 44)), Token(name='WORD', text='obciążenia', span=(45, 55)), Token(name='DOT', text='.', span=(55, 56))])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    Token = namedtuple('Token', 'name text span')\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        pat_list = []\n",
    "        for tok, pat in self.tokens:\n",
    "            pat_list.append('(?P<%s>%s)' % (tok, pat))\n",
    "            self.re = re.compile('|'.join(pat_list))\n",
    "\n",
    "    def iter_tokens(self, input, ignore_ws=True):\n",
    "        for match in self.re.finditer(input):\n",
    "            if ignore_ws and match.lastgroup == 'WHITESPACE':\n",
    "                continue\n",
    "            yield Tokenizer.Token(match.lastgroup, match.group(0), match.span(0))\n",
    "    \n",
    "    def iter_sentences(self, input, ignore_ws=True):\n",
    "        for sentence in re.compile('[.;!?] ').split(input):\n",
    "            yield ('SENTENCE', list(self.iter_tokens(sentence, ignore_ws)))\n",
    "            \n",
    "    def tokenize(self, input, ignore_ws=True):\n",
    "        return list(iter_sentences(input, ignore_ws))\n",
    "\n",
    "# test program\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    TOKENS = [\n",
    "        ('NIL'        , r\"nil|\\'()\"),\n",
    "        ('TRUE'       , r'true|#t'),\n",
    "        ('FALSE'      , r'false|#f'),\n",
    "        ('NUMBER'     , r'\\d+'),\n",
    "        ('STRING'     , r'\"(\\\\.|[^\"])*\"'),\n",
    "        ('WORD'     , r'[A-ZĄĆĘŁŃÓŚŹŻa-ząćęłńóśźż]+'),\n",
    "        ('QUOTE'      , r\"'\"),\n",
    "        ('LPAREN'     , r'\\('),\n",
    "        ('RPAREN'     , r'\\)'),\n",
    "        ('DOT'        , r'\\.'),\n",
    "        ('WHITESPACE' , r'\\w+'),\n",
    "    ]\n",
    "\n",
    "    for t in Tokenizer(TOKENS).iter_sentences('Linux 5.9 miał być mniejszą aktualizacją niż ostatecznie jest. Na większą liczbę nowinek nikt chyba jednak nie będzie narzekać. Szczególnie, że są to nowinki nie bez znaczenia, takie jak (dodana po ponad 5 latach prób) obsługa FSGSBASE, oznaczająca spory wzrost wydajności – zarówno w przypadku platform opartych na procesorach Intela, jak i tych, których sercami są układy AMD. Najbardziej odczujesz to w sytuacjach dużego obciążenia.'):\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy\n",
    "\n",
    "def load_poleval2019():\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'utf-8'\n",
    "        data = u.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality(data):\n",
    "    return {'data': len(data)}\n",
    "\n",
    "def class_frequencies(data):\n",
    "    class_freq = {}\n",
    "    for d in data:\n",
    "        if d[-1] not in class_freq:\n",
    "            class_freq[d[-1]] = 1\n",
    "        else:\n",
    "            class_freq[d[-1]] += 1\n",
    "    return class_freq\n",
    "\n",
    "def class_balance(data):\n",
    "    freq = class_frequencies(data)\n",
    "    total = sum([v for k, v in freq.items()])\n",
    "    return {k: round(float(v)*100/total, 2) for k, v in freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample data:\n['Dla mnie faworytem do tytułu będzie Cracovia. Zobaczymy, czy typ się sprawdzi.', '0']\nTotal: {'data': 11041}\nFreq: {'0': 10056, '2': 707, '1': 278}\nBalance: {'0': 91.08, '2': 6.4, '1': 2.52}\n"
     ]
    }
   ],
   "source": [
    "def visualize_quality(loader):\n",
    "    data = loader()\n",
    "    cls = [d[-1] for d in data]\n",
    "    print('Sample data:')\n",
    "    print(data[0])\n",
    "\n",
    "    print('Total: {}'.format(cardinality(data)))\n",
    "    print('Freq: {}'.format(class_frequencies(data)))\n",
    "    print('Balance: {}'.format(class_balance(data)))\n",
    "\n",
    "visualize_quality(load_poleval2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.30 seconds)\n",
      "Writing emoji data to /home/karol/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "from twitter_preprocessor import TwitterPreprocessor\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "def save_sentences(loader, sentences, labels):\n",
    "    data = loader()\n",
    "    with open(sentences, \"w+\") as f:\n",
    "        for index, text in enumerate(data):\n",
    "            tweet = TwitterPreprocessor(text[0])\n",
    "            tweet.remove_mentions().remove_urls().remove_hashtags()\n",
    "            tweetText = demoji.replace(tweet.text, \"\")\n",
    "            tweetText = tweetText.replace(\"\\\\\", \"\")\n",
    "            tweetText = tweetText.replace(\":)\", \"\")\n",
    "            tweetText = tweetText.replace(\";)\", \"\")\n",
    "            tweetText = tweetText.replace(\":-)\", \"\")\n",
    "            tweetText = tweetText.replace(\";-)\", \"\")\n",
    "            tweetText = tweetText.replace(\":D\", \"\")\n",
    "            tweetText = tweetText.replace(\":-D\", \"\")\n",
    "            f.write(tweetText + ' # ' + '\\n')\n",
    "        \n",
    "    with open(labels, \"w+\") as f:\n",
    "        for index, text in enumerate(data):\n",
    "            f.write(text[1] + '\\n')\n",
    "\n",
    "save_sentences(load_poleval2019, \"tweets.txt\", \"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def write_part_of_speech(readfile, filewrite, parts_of_speech):\n",
    "    tree = ET.parse(readfile)\n",
    "    chunklist = tree.getroot()\n",
    "    with open(filewrite, \"w+\") as f:\n",
    "        tweet = []\n",
    "        for token in chunklist.iter('tok'):\n",
    "            lex = token.find('lex')\n",
    "            base = lex.find('base').text\n",
    "            if base == '#':\n",
    "                tweetStr = ' '.join(tweet)\n",
    "                f.write(tweetStr + '\\n')\n",
    "                tweet = []\n",
    "            else:\n",
    "                ctag = lex.find('ctag').text\n",
    "                pos = ctag.split(':')[0]\n",
    "                if pos in parts_of_speech:\n",
    "                    tweet.append(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = ['subst', 'depr']\n",
    "write_part_of_speech(\"WCRFT2.xml\", 'WCRFT2Rzecz.txt', parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = ['adj', 'adja', 'adjp', 'adjc']\n",
    "write_part_of_speech(\"WCRFT2.xml\", 'WCRFT2Przym.txt', parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = ['fin', 'bedzie', 'aglt', 'praet', 'impt', 'imps', 'inf', 'pcon', 'pant', 'ger', 'pact', 'ppas', 'winien']\n",
    "write_part_of_speech(\"WCRFT2.xml\", 'WCRFT2Czas.txt', parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = ['NOUN', 'PROPN']\n",
    "write_part_of_speech(\"Spacy.xml\", 'SpacyRzecz.txt', parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = ['ADJ']\n",
    "write_part_of_speech(\"Spacy.xml\", 'SpacyPrzym.txt', parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = ['VERB']\n",
    "write_part_of_speech(\"Spacy.xml\", 'SpacyCzas.txt', parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def test_classifier(text_file):\n",
    "    columns = ['tweet', 'target']\n",
    "    rows = []\n",
    "    with open(text_file) as texts, open(\"labels.txt\") as labels:\n",
    "        for text, label in zip(texts, labels):\n",
    "            rows.append([text.rstrip('\\n'), label.rstrip('\\n')])\n",
    "\n",
    "    data = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    categories = ['0', '1', '2']\n",
    "\n",
    "    print(categories)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', ComplementNB(fit_prior=True, class_prior=None)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),  # unigrams or bigrams\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__alpha': (0.3, 0.6, 1.0)\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1_micro')\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(list(data.tweet), list(data.target))\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0', '1', '2']\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000, 10000, 50000), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': (0.3, 0.6, 1.0)}\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:   30.5s finished\n",
      "done in 30.544s\n",
      "\n",
      "Best score: 0.881\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1.0\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "test_classifier(\"MorphoDitaCzas.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0', '1', '2']\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000, 10000, 50000), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': (0.3, 0.6, 1.0)}\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:   21.2s finished\n",
      "done in 21.303s\n",
      "\n",
      "Best score: 0.885\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1.0\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "test_classifier(\"MorphoDitaPrzym.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0', '1', '2']\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000, 10000, 50000), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': (0.3, 0.6, 1.0)}\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:   45.5s finished\n",
      "done in 45.643s\n",
      "\n",
      "Best score: 0.879\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1.0\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "test_classifier(\"MorphoDitaRzecz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"WCRFT2Czas.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"WCRFT2Przym.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"WCRFT2Rzecz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"SpacyRzecz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"SpacyPrzym.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"SpacyCzas.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(\"tweets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}